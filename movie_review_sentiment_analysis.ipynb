{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm to tell the Apple company and apple the fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#function to clean the text\n",
    "def wikipedia_to_sents(url):\n",
    "    soup=BeautifulSoup(urlopen(url))\n",
    "    #drop the references []\n",
    "    def drop_refs(s):\n",
    "        return ''.join(re.split('\\[\\d+\\]', s))\n",
    "    paragraphs=[drop_refs(p.text) for p in soup.find_all('p')]\n",
    "    return [s.text for paragraph in paragraphs for s in nlp(paragraph).sents if len(s)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_sents=wikipedia_to_sents(\"http://en.wikipedia.org/wiki/Apple\")\n",
    "company_sents=wikipedia_to_sents(\"http://en.wikipedia.org/wiki/Apple_Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Apple's software includes the macOS, iOS, iPadOS, watchOS, and tvOS operating systems, the iTunes media player, the Safari web browser, the Shazam acoustic fingerprint utility, and the iLife and iWork creativity and productivity suites, as well as professional applications like Final Cut Pro, Logic Pro, and Xcode.\",\n",
       " 'Its online services include the iTunes Store, the iOS App Store, Mac App Store, Apple Music, Apple TV+, iMessage, and iCloud.',\n",
       " 'Other services include Apple Store, Genius Bar, AppleCare, Apple Pay, Apple Pay Cash, and Apple Card.\\n',\n",
       " \"Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976 to develop and sell Wozniak's Apple\",\n",
       " 'I personal computer, though Wayne sold his share back within 12 days.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_sents[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple trees are large if grown from seed.  ',\n",
       " 'Generally, apple cultivars are propagated by grafting onto rootstocks, which control the size of the resulting tree.',\n",
       " 'There are more than 7,500 known cultivars of apples, resulting in a range of desired characteristics.',\n",
       " 'Different cultivars are bred for various tastes and use, including cooking, eating raw and cider production.',\n",
       " 'Trees and fruit are prone to a number of fungal, bacterial and pest problems, which can be controlled by a number of organic and non-organic means.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_sents[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1063, 4691)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words_vectorizer=CountVectorizer()\n",
    "counts=bag_of_words_vectorizer.fit_transform(fruit_sents+company_sents)\n",
    "#row is the number of documents, column is the number of words, each value inside the matrix is the frequency\n",
    "#of that word in that document\n",
    "print(counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#count is a sparse matrix\n",
    "print(counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1379)\t1\n",
      "  (0, 2612)\t1\n",
      "  (0, 4358)\t1\n",
      "  (0, 737)\t1\n",
      "  (0, 3292)\t1\n",
      "  (0, 1837)\t1\n",
      "  (0, 1471)\t1\n",
      "  (0, 2335)\t1\n",
      "  (0, 441)\t2\n",
      "  (0, 392)\t3\n",
      "  (1, 1899)\t1\n",
      "  (1, 2163)\t1\n",
      "  (1, 3975)\t1\n",
      "  (1, 1982)\t1\n",
      "  (1, 4600)\t1\n",
      "  (1, 2760)\t1\n",
      "  (1, 4247)\t2\n",
      "  (1, 403)\t1\n",
      "  (1, 4640)\t1\n",
      "  (1, 1171)\t1\n",
      "  (1, 466)\t2\n",
      "  (1, 4359)\t1\n",
      "  (1, 2612)\t1\n",
      "  (1, 441)\t1\n",
      "  (2, 4302)\t1\n",
      "  :\t:\n",
      "  (1061, 441)\t1\n",
      "  (1062, 1457)\t1\n",
      "  (1062, 3575)\t1\n",
      "  (1062, 990)\t1\n",
      "  (1062, 223)\t1\n",
      "  (1062, 3498)\t1\n",
      "  (1062, 2190)\t1\n",
      "  (1062, 1342)\t1\n",
      "  (1062, 93)\t1\n",
      "  (1062, 2908)\t1\n",
      "  (1062, 86)\t1\n",
      "  (1062, 235)\t1\n",
      "  (1062, 308)\t1\n",
      "  (1062, 1760)\t1\n",
      "  (1062, 1827)\t1\n",
      "  (1062, 619)\t1\n",
      "  (1062, 731)\t1\n",
      "  (1062, 98)\t1\n",
      "  (1062, 4301)\t1\n",
      "  (1062, 4582)\t1\n",
      "  (1062, 2163)\t1\n",
      "  (1062, 4247)\t2\n",
      "  (1062, 403)\t1\n",
      "  (1062, 737)\t1\n",
      "  (1062, 392)\t1\n"
     ]
    }
   ],
   "source": [
    "#show the non-zero values\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another similar method is HashingVectorizer, which uses hashing map to store counts but the feature names. Faster but if you need feature names for futher use, have to use CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer is better than CountVectorizer and HashingVectorizer. The last two just counts the frequency of words. But we would like to find words that are common in one document, but not common in all of them. That is the features we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhimin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['future', 'gb', 'generally', 'generation', 'glass']\n",
      "  (0, 274)\t0.47762218446685034\n",
      "  (0, 205)\t0.5486393384553372\n",
      "  (0, 165)\t0.538625607528242\n",
      "  (0, 99)\t0.4251521846577247\n",
      "  (1, 296)\t0.4741198372432903\n",
      "  (1, 275)\t0.46428534408552163\n",
      "  (1, 165)\t0.5289846402305953\n",
      "  (1, 108)\t0.5289846402305953\n",
      "  (2, 291)\t0.427962505055314\n",
      "  (2, 274)\t0.3725660414907787\n",
      "  (2, 242)\t0.42015136015167814\n",
      "  (2, 165)\t0.42015136015167814\n",
      "  (2, 96)\t0.3853065567606607\n",
      "  (2, 50)\t0.42015136015167814\n",
      "  (3, 299)\t0.34810201458503676\n",
      "  (3, 179)\t0.4056929800488752\n",
      "  (3, 108)\t0.4253863131473256\n",
      "  (3, 88)\t0.4253863131473256\n",
      "  (3, 87)\t0.4056929800488752\n",
      "  (3, 30)\t0.433294782444718\n",
      "  (4, 122)\t0.6276081216589223\n",
      "  (4, 88)\t0.7785294121789872\n",
      "  (5, 275)\t0.5386411414657952\n",
      "  (5, 146)\t0.5772654967963395\n",
      "  (5, 108)\t0.6137020994985275\n",
      "  :\t:\n",
      "  (1056, 70)\t0.29366595072755247\n",
      "  (1056, 67)\t0.2783004379195872\n",
      "  (1056, 23)\t0.2620450700957848\n",
      "  (1056, 3)\t0.30992131855135485\n",
      "  (1057, 261)\t0.37677035991340574\n",
      "  (1057, 240)\t0.926306696451409\n",
      "  (1058, 198)\t0.4569058715644925\n",
      "  (1058, 139)\t0.38935316199769465\n",
      "  (1058, 134)\t0.46415415774405155\n",
      "  (1058, 31)\t0.46415415774405155\n",
      "  (1058, 24)\t0.4569058715644925\n",
      "  (1059, 201)\t0.7741689694495038\n",
      "  (1059, 67)\t0.6329789939180392\n",
      "  (1060, 167)\t0.460842319598661\n",
      "  (1060, 113)\t0.44984935173510443\n",
      "  (1060, 24)\t0.532037882598801\n",
      "  (1060, 6)\t0.5497232109800498\n",
      "  (1061, 225)\t0.4736874711977109\n",
      "  (1061, 208)\t0.3802961532602151\n",
      "  (1061, 192)\t0.4736874711977109\n",
      "  (1061, 168)\t0.4143775548791916\n",
      "  (1061, 58)\t0.4846714733594925\n",
      "  (1062, 28)\t0.6510411332707399\n",
      "  (1062, 21)\t0.517419895942362\n",
      "  (1062, 16)\t0.5553576271849936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "ng_tfidf=TfidfVectorizer(max_features=300, ngram_range=(1,2), \n",
    "                        stop_words=STOP_WORDS.union({'apple', 'apples','Apple'}))\n",
    "ng_tfidf=ng_tfidf.fit(fruit_sents+company_sents)\n",
    "print(ng_tfidf.get_feature_names()[100:105])\n",
    "print(ng_tfidf.transform(fruit_sents+company_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carry', 'carry', 'carry', 'carry']\n"
     ]
    }
   ],
   "source": [
    "print([w.lemma_ for w in nlp('carry carries carrying carried')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '  ', '   billion', '   gb', '   million', '\"', '\" \"', '\" ,', '\" .', '$', '%', '(', ')', ') ,', ') .', ',', ', \"', ', ,', ', 2012', ', 2016', ', announce', ', company', ', feature', ', include', ', introduce', ', iphone', ', job', ', release', '-', '- -', '- generation', '.', '. \\n', '1', '10', '100', '11', '12', '12 ,', '2', '2007', '2008', '2009', '2010', '2010 ,', '2011', '2011 ,', '2012', '2012 ,', '2013', '2013 ,', '2014', '2015', '2015 ,', '2016', '2016 ,', '2017', '2017 ,', '2018', '2019', '2019 ,', '2020', '3', '30', '4', '5', '6', '7', '8', ':', ';', ']', 'acquire', 'add', 'allow', 'announce', 'app', 'app store', 'application', 'april', 'august', 'base', 'begin', 'billion', 'brand', 'build', 'business', 'camera', 'campus', 'cause', 'center', 'century', 'ceo', 'change', 'china', 'claim', 'color', 'come', 'company', 'company .', 'computer', 'computer ,', 'consumer', 'continue', 'control', 'cook', 'country', 'create', 'cultivar', 'datum', 'day', 'december', 'design', 'desktop', 'develop', 'development', 'device', 'different', 'digital', 'disease', 'display', 'early', 'eat', 'effort', 'electronic', 'employee', 'end', 'energy', 'europe', 'event', 'executive', 'facility', 'feature', 'find', 'focus', 'follow', 'form', 'fruit', 'gb', 'generally', 'generation', 'glass', 'good', 'government', 'group', 'grow', 'health', 'help', 'high', 'ii', 'imac', 'improve', 'inc', 'inc .', 'include', 'increase', 'individual', 'information', 'introduce', 'ios', 'ipad', 'iphone', 'iphone 11', 'ipod', 'irish', 'issue', 'itunes', 'january', 'job', 'july', 'june', 'know', 'large', 'late', 'later', 'launch', 'lead', 'leave', 'life', 'line', 'location', 'logo', 'long', 'low', 'mac', 'mac os', 'macintosh', 'major', 'malus', 'manufacturer', 'manufacturing', 'march', 'market', 'market .', 'medium', 'microsoft', 'million', 'model', 'model ,', 'modern', 'month', 'music', 'need', 'new', 'north', 'november', 'number', 'october', 'offer', 'old', 'open', 'operating', 'original', 'os', 'pay', 'people', 'period', 'personal', 'phone', 'plant', 'platform', 'player', 'power', 'practice', 'president', 'price', 'pro', 'processor', 'produce', 'product', 'product .', 'production', 'profit', 'project', 'provide', 'purchase', 'rank', 'receive', 'record', 'release', 'renewable', 'replace', 'report', 'research', 'result', 'retail', 'revenue', 'rootstock', 'run', 'sale', 'samsung', 'sculley', 'second', 'security', 'seed', 'sell', 'september', 'series', 'service', 'share', 'sign', 'significant', 'siri', 'size', 'smartphone', 'software', 'state', 'states', 'steve', 'storage', 'store', 'store ,', 'success', 'support', 'system', 'tax', 'technology', 'time', 'total', 'touch', 'tree', 'tv', 'u.s', 'u.s .', 'united', 'united states', 'update', 'user', 'version', 'video', 'watch', 'work', 'worker', 'world', 'worldwide', 'wozniak', 'x', 'year', 'year ,', 'â€”']\n"
     ]
    }
   ],
   "source": [
    "#function for stemming\n",
    "\n",
    "def tokenize_lemma(text):\n",
    "    return [w.lemma_.lower() for w in nlp(text)]\n",
    "\n",
    "#since words are stemmed, it is better to stem stopwords as well.\n",
    "stop_words_lemma=set(tokenize_lemma(' '.join(STOP_WORDS))) \n",
    "ng_stem_tfidf=TfidfVectorizer(max_features=300, ngram_range=(1,2),\n",
    "                             stop_words=stop_words_lemma.union({'apple'}),\n",
    "                             tokenizer=tokenize_lemma)\n",
    "ng_stem_tfidf=ng_stem_tfidf.fit(fruit_sents+company_sents)\n",
    "\n",
    "ng_stem_vocab=ng_stem_tfidf.get_feature_names()\n",
    "print(ng_stem_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "classifier=MultinomialNB()\n",
    "pipe=Pipeline([('vectorizer',ng_stem_tfidf), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fruit_sents+company_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=['apple']*len(fruit_sents)+['Apple']*len(company_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=300, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=T...True, vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'Apple', 'Apple', 'Apple', 'Apple'], dtype='<U5')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(['I like to eat apple', \n",
    "              'I like apple juice',\n",
    "              'I looked up the recipe on my Apple phone', \n",
    "              'Steve Jobs is the CEO of apple', \n",
    "             'I bought some apple stocks',\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some are not correct. Because the sample size is small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
